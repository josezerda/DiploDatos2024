{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from ml.data import create_lineal_data\n",
    "from ml.visualization import decision_boundary\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de coste y gradiente\n",
    "\n",
    "## Generación de datos\n",
    "\n",
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Para hacer más determinística la generación de datos\n",
    "\n",
    "samples_per_class = 5\n",
    "\n",
    "Xa = np.c_[create_lineal_data(0.75, 0.9, spread=0.2, data_size=samples_per_class)]\n",
    "Xb = np.c_[create_lineal_data(0.5, 0.75, spread=0.2, data_size=samples_per_class)]\n",
    "X_train = np.r_[Xa, Xb]\n",
    "y_train = np.r_[np.zeros(samples_per_class), np.ones(samples_per_class)]\n",
    "\n",
    "cmap_dots = ListedColormap(['tomato', 'dodgerblue'])\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_dots, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # Para hacer más determinística la generación de datos\n",
    "\n",
    "samples_per_class = 25\n",
    "\n",
    "Xa = np.c_[create_lineal_data(0.75, 0.9, spread=0.2, data_size=samples_per_class)]\n",
    "Xb = np.c_[create_lineal_data(0.5, 0.75, spread=0.2, data_size=samples_per_class)]\n",
    "X_val = np.r_[Xa, Xb]\n",
    "y_val = np.r_[np.zeros(samples_per_class), np.ones(samples_per_class)]\n",
    "\n",
    "cmap_dots = ListedColormap(['tomato', 'dodgerblue'])\n",
    "\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=cmap_dots, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logística\n",
    "\n",
    "### Función de coste y gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logloss(w, x, y):\n",
    "    m = y.shape[0]\n",
    "    y_hat = sigmoid(x.dot(w))\n",
    "    cost1 = np.log(y_hat).dot(y)\n",
    "    cost2 = np.log(1 - y_hat).dot(1 - y)\n",
    "    J = -(cost1 + cost2)\n",
    "\n",
    "    return J\n",
    "\n",
    "def logloss_gradient(w, x, y):\n",
    "    m = y.shape[0]\n",
    "    y_hat = sigmoid(x.dot(w))\n",
    "    gradient = np.dot(x.T, y_hat - y)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de optimización (descenso por la gradiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w, x_train, y_train, x_val, y_va, cost_function,\n",
    "                     cost_function_gradient, alpha=0.01, max_iter=1000):\n",
    "    train_costs = np.zeros(max_iter)\n",
    "    val_costs = np.zeros(max_iter)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        train_costs[iteration] = cost_function(w, x_train, y_train)\n",
    "        val_costs[iteration] = cost_function(w, x_val, y_val)\n",
    "        gradient = cost_function_gradient(w, x_train, y_train)\n",
    "        w = w - alpha * gradient\n",
    "    \n",
    "    return w, train_costs, val_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar el vector de bias a los ejemplos (bias trick)\n",
    "X_b_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_b_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
    "\n",
    "w0 = np.zeros(X_b_train.shape[1])  # Initial weights\n",
    "\n",
    "w, train_costs, val_costs = gradient_descent(w0, X_b_train, y_train, X_b_val, y_val,\n",
    "                                             logloss, logloss_gradient, max_iter=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactitud (entrenamiento vs validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (X_b_train.dot(w) >= 0.5).astype(np.int)  # Obtenemos las predicciones (como 0 o 1)\n",
    "accuracy = (y_train == y_pred).astype(np.int).sum() / y_train.shape[0]  # Calcular la exactitud\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy)\n",
    "\n",
    "y_pred = (X_b_val.dot(w) >= 0.5).astype(np.int)  # Obtenemos las predicciones (como 0 o 1)\n",
    "accuracy = (y_val == y_pred).astype(np.int).sum() / y_val.shape[0]  # Calcular la exactitud\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva de aprendizaje (entrenamiento vs validación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_costs, label=\"Datos de entrenamiento\")\n",
    "plt.plot(val_costs, label=\"Datos de validación\")\n",
    "plt.xlabel(\"Iteraciones\")\n",
    "plt.ylabel(\"Costo\")\n",
    "plt.title(\"Curva de aprendizaje\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frontera de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy, Z = decision_boundary(np.r_[X_train, X_val], w)\n",
    "\n",
    "cmap_back = ListedColormap(['lightcoral', 'skyblue'])\n",
    "cmap_dots = ['tomato', 'dodgerblue', 'red', 'darkslateblue']\n",
    "\n",
    "plt.figure(figsize=(6, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_back)\n",
    "\n",
    "for i in (0, 1):\n",
    "    plt.scatter(X_train[y_train==i, 0], X_train[y_train==i, 1], \n",
    "                color=cmap_dots[i], label='Entrenamiento clase %d' % i,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.scatter(X_val[y_val==i, 0], X_val[y_val==i, 1],\n",
    "                color=cmap_dots[i+2], label='Validación clase %d' % i,\n",
    "                edgecolor='k', s=20)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
