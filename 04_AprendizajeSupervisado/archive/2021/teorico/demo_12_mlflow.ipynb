{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: ML Flow\n",
    "\n",
    "[MLFlow](https://mlflow.org/) is a tool to keep track of experiments. Is easy to setup and you can keep track of different parameters and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X, y = data[\"data\"], data[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To get started, set the experiment name. Different runs of the same experiment should be saved under the same experiment name to make it possible to compare.\n",
    "\n",
    "You can log the parameters via `log_param(name, value)` (for single parameters) and `log_params(Dict[name, value])` for multiple parameters.\n",
    "\n",
    "You can log the metrics via `log_metric(name, value)`.\n",
    "\n",
    "For more possibilities check the [documentation](https://mlflow.org/docs/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"iris_experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    params = {\n",
    "        \"max_depth\": 1,\n",
    "        \"criterion\": \"entropy\"\n",
    "    }\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    prec, rec, fscore, _ = precision_recall_fscore_support(y_test, clf.predict(X_test), average=\"macro\")\n",
    "    mlflow.log_metric(\"precision\", prec)\n",
    "    mlflow.log_metric(\"recall\", rec)\n",
    "    mlflow.log_metric(\"f1-score\", fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Experiment\n",
    "\n",
    "We run two experiment to see how they behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    params = {\n",
    "        \"max_depth\": 6,\n",
    "        \"criterion\": \"gini\"\n",
    "    }\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    clf = DecisionTreeClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    prec, rec, fscore, _ = precision_recall_fscore_support(y_test, clf.predict(X_test), average=\"macro\")\n",
    "    mlflow.log_metric(\"precision\", prec)\n",
    "    mlflow.log_metric(\"recall\", rec)\n",
    "    mlflow.log_metric(\"f1-score\", fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "\n",
    "To check the results you should run `mlflow ui` from your terminal, or you can run it from here (warning, if you run the cell you need to stop the cell before running anything else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
